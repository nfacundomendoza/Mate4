import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class LinearRegressionGD:
    def __init__(self, learning_rate=0.01, n_iter=1000):
        self.learning_rate = learning_rate
        self.n_iter = n_iter
        self.weights = None
        self.bias = None
        self.loss_history = []
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        print("üîÑ Ejecutando Descenso del Gradiente...")
        for i in range(self.n_iter):
            # Predicciones
            y_pred = np.dot(X, self.weights) + self.bias
            
            # Gradientes
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # Actualizar par√°metros
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # Calcular p√©rdida
            loss = np.mean((y_pred - y)**2)
            self.loss_history.append(loss)
            
            # Mostrar progreso cada 100 iteraciones
            if i % 200 == 0:
                print(f"Iteraci√≥n {i:4d}, P√©rdida (MSE): {loss:.6f}")
    
    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

# 1. CARGAR DATOS
print("üìÅ Cargando dataset Iris...")
ds = pd.read_csv("iris.data", header=None, names=[
    "sepal_length", "sepal_width", "petal_length", "petal_width", "class"
])
print(f"Dataset cargado: {len(ds)} registros")

# 2. PREPARAR DATOS
print("\nüîß Preparando datos para regresi√≥n m√∫ltiple...")
X = ds[["sepal_length", "sepal_width", "petal_width"]].values
y = ds["petal_length"].values

print("Variables predictoras:")
print("- sepal_length (x‚ÇÅ)")
print("- sepal_width  (x‚ÇÇ)") 
print("- petal_width  (x‚ÇÉ)")
print(f"Variable respuesta: petal_length (Y)")

# 3. NORMALIZAR CARACTER√çSTICAS (importante para Descenso del Gradiente)
print("\nüìä Normalizando caracter√≠sticas...")
X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_normalized = (X - X_mean) / X_std

print("Medias antes de normalizar:", X_mean)
print("Desviaciones est√°ndar:", X_std)

# 4. ENTRENAR CON DESCENSO DEL GRADIENTE
print("\nüéØ Entrenando modelo con Descenso del Gradiente...")
modelo_gd = LinearRegressionGD(learning_rate=0.01, n_iter=1000)
modelo_gd.fit(X_normalized, y)

# 5. DESNORMALIZAR COEFICIENTES PARA INTERPRETACI√ìN
print("\nüîÑ Desnormalizando coeficientes...")
coef_desnormalizados = modelo_gd.weights / X_std
intercepto_desnormalizado = modelo_gd.bias - np.sum(coef_desnormalizados * X_mean)

# 6. MOSTRAR RESULTADOS FINALES
print("\n" + "="*60)
print("üìà RESULTADOS FINALES - DESCENSO DEL GRADIENTE")
print("="*60)
print(f"ECUACI√ìN DE REGRESI√ìN M√öLTIPLE:")
print(f"petal_length = {intercepto_desnormalizado:.4f} + ")
print(f"               ({coef_desnormalizados[0]:.4f} √ó sepal_length) + ")
print(f"               ({coef_desnormalizados[1]:.4f} √ó sepal_width) + ")
print(f"               ({coef_desnormalizados[2]:.4f} √ó petal_width)")

print(f"\nüìä Coeficientes finales:")
print(f"Œ≤‚ÇÄ (Intercepto): {intercepto_desnormalizado:.4f}")
print(f"Œ≤‚ÇÅ (sepal_length): {coef_desnormalizados[0]:.4f}")
print(f"Œ≤‚ÇÇ (sepal_width): {coef_desnormalizados[1]:.4f}")
print(f"Œ≤‚ÇÉ (petal_width): {coef_desnormalizados[2]:.4f}")

print(f"\nüìâ P√©rdida final (MSE): {modelo_gd.loss_history[-1]:.6f}")

# 7. GR√ÅFICO DE CONVERGENCIA (opcional)
plt.figure(figsize=(10, 6))
plt.plot(modelo_gd.loss_history)
plt.title('Convergencia del Descenso del Gradiente')
plt.xlabel('Iteraci√≥n')
plt.ylabel('P√©rdida (MSE)')
plt.grid(True)
plt.show()

print("\n‚úÖ Descenso del Gradiente completado exitosamente!")